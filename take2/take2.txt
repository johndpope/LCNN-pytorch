// sparse_conv2d_cuda.cpp
#include <torch/extension.h>

torch::Tensor sparse_conv2d_forward_cuda(
    torch::Tensor input,
    torch::Tensor dictionary,
    torch::Tensor lookup_indices,
    torch::Tensor lookup_coefficients,
    int stride, int padding);

#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x " must be a CUDA tensor")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")
#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)

torch::Tensor sparse_conv2d_forward(
    torch::Tensor input,
    torch::Tensor dictionary,
    torch::Tensor lookup_indices,
    torch::Tensor lookup_coefficients,
    int stride, int padding) {
    
    CHECK_INPUT(input);
    CHECK_INPUT(dictionary);
    CHECK_INPUT(lookup_indices);
    CHECK_INPUT(lookup_coefficients);

    return sparse_conv2d_forward_cuda(input, dictionary, lookup_indices, lookup_coefficients, stride, padding);
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("forward", &sparse_conv2d_forward, "Sparse Conv2d forward (CUDA)");
}
import torch
import torch.nn as nn
import torch.optim as optim
from torch.profiler import profile, record_function, ProfilerActivity
import numpy as np
import random
from torch.utils.data import DataLoader
from datasets import load_dataset
from torchvision import transforms

from lookup_conv2d import LookupConv2d
class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = LookupConv2d(3, 64, kernel_size=3, stride=1, padding=1, dictionary_size=100, sparsity=3)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(2, 2)
        self.fc = nn.Linear(64 * 16 * 16, 10)

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = x.view(-1, 64 * 16 * 16)
        x = self.fc(x)
        return x

model = MyModel().cuda()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
criterion = nn.CrossEntropyLoss()




# Load the dataset
dataset = load_dataset("lansinuote/gen.1.celeba")

# Define image transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize images to a standard size
    transforms.ToTensor(),  # Convert images to PyTorch tensors
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize
])

# Create a function to apply transformations
def transform_images(examples):
    examples['pixel_values'] = [transform(image.convert('RGB')) for image in examples['image']]
    return examples

# Apply the transformations to the dataset
dataset = dataset.map(transform_images, batched=True, remove_columns=['image'])

# Set the format of the dataset to PyTorch
dataset.set_format(type='torch', columns=['pixel_values', 'attributes'])

# Create DataLoaders
batch_size = 32

train_dataloader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(dataset['validation'], batch_size=batch_size)
test_dataloader = DataLoader(dataset['test'], batch_size=batch_size)

# Now you can use these DataLoaders in your training loop
# For example:


for epoch in range(100):
    for batch in train_dataloader:
        inputs = batch['pixel_values']
        labels = batch['attributes']
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # Optionally, you can add L1 regularization to encourage sparsity
        l1_lambda = 0.0001
        l1_norm = sum(p.abs().sum() for p in model.parameters())
        loss = loss + l1_lambda * l1_norm
import torch
import torch.nn as nn
from torch.autograd import Function
import sparse_conv2d_cuda

class SparseConv2dFunction(Function):
    @staticmethod
    def forward(ctx, input, dictionary, lookup_indices, lookup_coefficients, stride, padding):
        ctx.save_for_backward(input, dictionary, lookup_indices, lookup_coefficients)
        ctx.stride = stride
        ctx.padding = padding
        return sparse_conv2d_cuda.forward(input, dictionary, lookup_indices, lookup_coefficients, stride, padding)

    @staticmethod
    def backward(ctx, grad_output):
        input, dictionary, lookup_indices, lookup_coefficients = ctx.saved_tensors
        stride = ctx.stride
        padding = ctx.padding
        
        grad_input, grad_dictionary, grad_coefficients = sparse_conv2d_cuda.backward(
            grad_output.contiguous(), input, dictionary, lookup_indices, lookup_coefficients, stride, padding)
        
        return grad_input, grad_dictionary, None, grad_coefficients, None, None

class LookupConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dictionary_size=100, sparsity=3):
        super(LookupConv2d, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dictionary_size = dictionary_size
        self.sparsity = sparsity
        
        self.dictionary = nn.Parameter(torch.Tensor(dictionary_size, in_channels, kernel_size, kernel_size))
        self.register_buffer('lookup_indices', torch.zeros(out_channels, sparsity, dtype=torch.long))
        self.lookup_coefficients = nn.Parameter(torch.Tensor(out_channels, sparsity))
        
        self.reset_parameters()
    
    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.dictionary)
        nn.init.kaiming_uniform_(self.lookup_coefficients)
        self.lookup_indices.random_(0, self.dictionary_size)
    
    def forward(self, x):
        # Enforce sparsity before each forward pass
        self.enforce_sparsity()
        return SparseConv2dFunction.apply(
            x, self.dictionary, self.lookup_indices, self.lookup_coefficients,
            self.stride, self.padding)

    def extra_repr(self):
        return (f'in_channels={self.in_channels}, out_channels={self.out_channels}, '
                f'kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding}, '
                f'dictionary_size={self.dictionary_size}, sparsity={self.sparsity}')
    

    '''
    This enforce_sparsity method does the following:

    It sorts the absolute values of the lookup coefficients.
    It creates a mask for the top-k (where k is the sparsity) values.
    It zeros out the coefficients that are not in the top-k.
    It updates the lookup indices to match the new order of coefficients.

    By calling this method before each forward pass, we ensure that the sparsity constraint is always maintained.

'''
    def enforce_sparsity(self):
        with torch.no_grad():
            # Sort coefficients by magnitude
            sorted_coeff, sorted_indices = torch.sort(self.lookup_coefficients.abs(), dim=1, descending=True)
            
            # Create a mask for the top-k values
            mask = torch.zeros_like(self.lookup_coefficients, dtype=torch.bool)
            mask.scatter_(1, sorted_indices[:, :self.sparsity], 1)
            
            # Zero out coefficients that are not in the top-k
            self.lookup_coefficients.masked_fill_(~mask, 0)
            
            # Update lookup indices
            new_indices = torch.gather(self.lookup_indices, 1, sorted_indices)
            self.lookup_indices.copy_(new_indices)


from setuptools import setup
from torch.utils.cpp_extension import BuildExtension, CUDAExtension

setup(
    name='sparse_conv2d_cuda',
    ext_modules=[
        CUDAExtension('sparse_conv2d_cuda', [
            'sparse_conv2d_cuda.cpp',
            'sparse_conv2d_kernel.cu',
        ]),
    ],
    cmdclass={
        'build_ext': BuildExtension
    })
// sparse_conv2d_kernel.cu
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__global__ void sparse_conv2d_forward_kernel(
    const scalar_t* input,
    const scalar_t* dictionary,
    const int64_t* lookup_indices,
    const scalar_t* lookup_coefficients,
    scalar_t* output,
    int batch_size, int in_channels, int in_height, int in_width,
    int out_channels, int out_height, int out_width,
    int kernel_size, int stride, int padding,
    int dictionary_size, int sparsity) {
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * out_channels * out_height * out_width) return;

    int w_out = idx % out_width;
    int h_out = (idx / out_width) % out_height;
    int c_out = (idx / (out_width * out_height)) % out_channels;
    int n = idx / (out_width * out_height * out_channels);

    scalar_t sum = 0;

    for (int s = 0; s < sparsity; ++s) {
        int dict_idx = lookup_indices[c_out * sparsity + s];
        scalar_t coeff = lookup_coefficients[c_out * sparsity + s];

        for (int ic = 0; ic < in_channels; ++ic) {
            for (int kh = 0; kh < kernel_size; ++kh) {
                for (int kw = 0; kw < kernel_size; ++kw) {
                    int h_in = h_out * stride - padding + kh;
                    int w_in = w_out * stride - padding + kw;
                    
                    if (h_in >= 0 && h_in < in_height && w_in >= 0 && w_in < in_width) {
                        int input_idx = ((n * in_channels + ic) * in_height + h_in) * in_width + w_in;
                        int dict_idx_full = ((dict_idx * in_channels + ic) * kernel_size + kh) * kernel_size + kw;
                        
                        sum += input[input_idx] * dictionary[dict_idx_full] * coeff;
                    }
                }
            }
        }
    }

    output[idx] = sum;
}

torch::Tensor sparse_conv2d_forward_cuda(
    torch::Tensor input,
    torch::Tensor dictionary,
    torch::Tensor lookup_indices,
    torch::Tensor lookup_coefficients,
    int stride, int padding) {

    const auto batch_size = input.size(0);
    const auto in_channels = input.size(1);
    const auto in_height = input.size(2);
    const auto in_width = input.size(3);
    const auto kernel_size = dictionary.size(2);
    const auto out_channels = lookup_indices.size(0);
    const auto dictionary_size = dictionary.size(0);
    const auto sparsity = lookup_indices.size(1);

    const auto out_height = (in_height + 2 * padding - kernel_size) / stride + 1;
    const auto out_width = (in_width + 2 * padding - kernel_size) / stride + 1;

    auto output = torch::zeros({batch_size, out_channels, out_height, out_width}, input.options());

    const int threads = 1024;
    const int blocks = (batch_size * out_channels * out_height * out_width + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES(input.type(), "sparse_conv2d_forward_cuda", ([&] {
        sparse_conv2d_forward_kernel<scalar_t><<<blocks, threads>>>(
            input.data<scalar_t>(),
            dictionary.data<scalar_t>(),
            lookup_indices.data<int64_t>(),
            lookup_coefficients.data<scalar_t>(),
            output.data<scalar_t>(),
            batch_size, in_channels, in_height, in_width,
            out_channels, out_height, out_width,
            kernel_size, stride, padding,
            dictionary_size, sparsity
        );
    }));

    return output;
}

template <typename scalar_t>
__global__ void sparse_conv2d_backward_input_kernel(
    const scalar_t* grad_output,
    const scalar_t* dictionary,
    const int64_t* lookup_indices,
    const scalar_t* lookup_coefficients,
    scalar_t* grad_input,
    int batch_size, int in_channels, int in_height, int in_width,
    int out_channels, int out_height, int out_width,
    int kernel_size, int stride, int padding,
    int dictionary_size, int sparsity) {
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * in_channels * in_height * in_width) return;

    int w_in = idx % in_width;
    int h_in = (idx / in_width) % in_height;
    int c_in = (idx / (in_width * in_height)) % in_channels;
    int n = idx / (in_width * in_height * in_channels);

    scalar_t sum = 0;

    for (int oc = 0; oc < out_channels; ++oc) {
        for (int s = 0; s < sparsity; ++s) {
            int dict_idx = lookup_indices[oc * sparsity + s];
            scalar_t coeff = lookup_coefficients[oc * sparsity + s];

            for (int kh = 0; kh < kernel_size; ++kh) {
                for (int kw = 0; kw < kernel_size; ++kw) {
                    int h_out = (h_in + padding - kh) / stride;
                    int w_out = (w_in + padding - kw) / stride;
                    
                    if (h_out >= 0 && h_out < out_height && w_out >= 0 && w_out < out_width &&
                        (h_in + padding - kh) % stride == 0 && (w_in + padding - kw) % stride == 0) {
                        int grad_output_idx = ((n * out_channels + oc) * out_height + h_out) * out_width + w_out;
                        int dict_idx_full = ((dict_idx * in_channels + c_in) * kernel_size + kh) * kernel_size + kw;
                        
                        sum += grad_output[grad_output_idx] * dictionary[dict_idx_full] * coeff;
                    }
                }
            }
        }
    }

    grad_input[idx] = sum;
}

template <typename scalar_t>
__global__ void sparse_conv2d_backward_dictionary_kernel(
    const scalar_t* grad_output,
    const scalar_t* input,
    const int64_t* lookup_indices,
    const scalar_t* lookup_coefficients,
    scalar_t* grad_dictionary,
    int batch_size, int in_channels, int in_height, int in_width,
    int out_channels, int out_height, int out_width,
    int kernel_size, int stride, int padding,
    int dictionary_size, int sparsity) {
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= dictionary_size * in_channels * kernel_size * kernel_size) return;

    int kw = idx % kernel_size;
    int kh = (idx / kernel_size) % kernel_size;
    int c_in = (idx / (kernel_size * kernel_size)) % in_channels;
    int dict_idx = idx / (in_channels * kernel_size * kernel_size);

    scalar_t sum = 0;

    for (int n = 0; n < batch_size; ++n) {
        for (int oc = 0; oc < out_channels; ++oc) {
            for (int s = 0; s < sparsity; ++s) {
                if (lookup_indices[oc * sparsity + s] == dict_idx) {
                    scalar_t coeff = lookup_coefficients[oc * sparsity + s];
                    
                    for (int h_out = 0; h_out < out_height; ++h_out) {
                        for (int w_out = 0; w_out < out_width; ++w_out) {
                            int h_in = h_out * stride - padding + kh;
                            int w_in = w_out * stride - padding + kw;
                            
                            if (h_in >= 0 && h_in < in_height && w_in >= 0 && w_in < in_width) {
                                int input_idx = ((n * in_channels + c_in) * in_height + h_in) * in_width + w_in;
                                int grad_output_idx = ((n * out_channels + oc) * out_height + h_out) * out_width + w_out;
                                
                                sum += grad_output[grad_output_idx] * input[input_idx] * coeff;
                            }
                        }
                    }
                }
            }
        }
    }

    grad_dictionary[idx] = sum;
}

template <typename scalar_t>
__global__ void sparse_conv2d_backward_coefficients_kernel(
    const scalar_t* grad_output,
    const scalar_t* input,
    const scalar_t* dictionary,
    const int64_t* lookup_indices,
    scalar_t* grad_coefficients,
    int batch_size, int in_channels, int in_height, int in_width,
    int out_channels, int out_height, int out_width,
    int kernel_size, int stride, int padding,
    int dictionary_size, int sparsity) {
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= out_channels * sparsity) return;

    int s = idx % sparsity;
    int oc = idx / sparsity;

    int dict_idx = lookup_indices[idx];
    scalar_t sum = 0;

    for (int n = 0; n < batch_size; ++n) {
        for (int h_out = 0; h_out < out_height; ++h_out) {
            for (int w_out = 0; w_out < out_width; ++w_out) {
                int grad_output_idx = ((n * out_channels + oc) * out_height + h_out) * out_width + w_out;
                scalar_t grad = grad_output[grad_output_idx];

                for (int c_in = 0; c_in < in_channels; ++c_in) {
                    for (int kh = 0; kh < kernel_size; ++kh) {
                        for (int kw = 0; kw < kernel_size; ++kw) {
                            int h_in = h_out * stride - padding + kh;
                            int w_in = w_out * stride - padding + kw;
                            
                            if (h_in >= 0 && h_in < in_height && w_in >= 0 && w_in < in_width) {
                                int input_idx = ((n * in_channels + c_in) * in_height + h_in) * in_width + w_in;
                                int dict_idx_full = ((dict_idx * in_channels + c_in) * kernel_size + kh) * kernel_size + kw;
                                
                                sum += grad * input[input_idx] * dictionary[dict_idx_full];
                            }
                        }
                    }
                }
            }
        }
    }

    grad_coefficients[idx] = sum;
}

// Forward declaration of the CUDA functions
torch::Tensor sparse_conv2d_forward_cuda(
    torch::Tensor input,
    torch::Tensor dictionary,
    torch::Tensor lookup_indices,
    torch::Tensor lookup_coefficients,
    int stride, int padding);

std::vector<torch::Tensor> sparse_conv2d_backward_cuda(
    torch::Tensor grad_output,
    torch::Tensor input,
    torch::Tensor dictionary,
    torch::Tensor lookup_indices,
    torch::Tensor lookup_coefficients,
    int stride, int padding);

// Implementation of the CUDA functions
torch::Tensor sparse_conv2d_forward_cuda(
    torch::Tensor input,
    torch::Tensor dictionary,
    torch::Tensor lookup_indices,
    torch::Tensor lookup_coefficients,
    int stride, int padding) {
    // ... (same as before)
}

std::vector<torch::Tensor> sparse_conv2d_backward_cuda(
    torch::Tensor grad_output,
    torch::Tensor input,
    torch::Tensor dictionary,
    torch::Tensor lookup_indices,
    torch::Tensor lookup_coefficients,
    int stride, int padding) {

    const auto batch_size = input.size(0);
    const auto in_channels = input.size(1);
    const auto in_height = input.size(2);
    const auto in_width = input.size(3);
    const auto kernel_size = dictionary.size(2);
    const auto out_channels = lookup_indices.size(0);
    const auto dictionary_size = dictionary.size(0);
    const auto sparsity = lookup_indices.size(1);

    const auto out_height = grad_output.size(2);
    const auto out_width = grad_output.size(3);

    auto grad_input = torch::zeros_like(input);
    auto grad_dictionary = torch::zeros_like(dictionary);
    auto grad_coefficients = torch::zeros_like(lookup_coefficients);

    const int threads = 1024;

    const int blocks_input = (batch_size * in_channels * in_height * in_width + threads - 1) / threads;
    const int blocks_dictionary = (dictionary_size * in_channels * kernel_size * kernel_size + threads - 1) / threads;
    const int blocks_coefficients = (out_channels * sparsity + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES(input.type(), "sparse_conv2d_backward_cuda", ([&] {
        sparse_conv2d_backward_input_kernel<scalar_t><<<blocks_input, threads>>>(
            grad_output.data<scalar_t>(),
            dictionary.data<scalar_t>(),
            lookup_indices.data<int64_t>(),
            lookup_coefficients.data<scalar_t>(),
            grad_input.data<scalar_t>(),
            batch_size, in_channels, in_height, in_width,
            out_channels, out_height, out_width,
            kernel_size, stride, padding,
            dictionary_size, sparsity
        );

        sparse_conv2d_backward_dictionary_kernel<scalar_t><<<blocks_dictionary, threads>>>(
            grad_output.data<scalar_t>(),
            input.data<scalar_t>(),
            lookup_indices.data<int64_t>(),
            lookup_coefficients.data<scalar_t>(),
            grad_dictionary.data<scalar_t>(),
            batch_size, in_channels, in_height, in_width,
            out_channels, out_height, out_width,
            kernel_size, stride, padding,
            dictionary_size, sparsity
        );

        sparse_conv2d_backward_coefficients_kernel<scalar_t><<<blocks_coefficients, threads>>>(
            grad_output.data<scalar_t>(),
            input.data<scalar_t>(),
            dictionary.data<scalar_t>(),
            lookup_indices.data<int64_t>(),
            grad_coefficients.data<scalar_t>(),
            batch_size, in_channels, in_height, in_width,
            out_channels, out_height, out_width,
            kernel_size, stride, padding,
            dictionary_size, sparsity
        );
    }));

    return {grad_input, grad_dictionary, grad_coefficients};
}

Update the PyTorch C++ extension (sparse_conv2d_cuda.cpp):

cppCopy#include <torch/extension.h>

torch::Tensor sparse_conv2d_forward_cuda(
    torch::Tensor input,
    torch::Tensor dictionary,
    torch::Tensor lookup_indices,
    torch::Tensor lookup_coefficients,
    int stride, int padding);

std::vector<torch::Tensor> sparse_conv2d_backward_cuda(
    torch::Tensor grad_output,
    torch::Tensor input,
    torch::Tensor dictionary,
    torch::Tensor lookup_indices,
    torch::Tensor lookup_coefficients,
    int stride, int padding);

#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x " must be a CUDA tensor")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")
#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)

torch::Tensor sparse_conv2d_forward(
    torch::Tensor input,
    torch::Tensor dictionary,
    torch::Tensor lookup_indices,
    torch::Tensor lookup_coefficients,
    int stride, int padding) {
    
    CHECK_INPUT(input);
    CHECK_INPUT(dictionary);
    CHECK_INPUT(lookup_indices);
    CHECK_INPUT(lookup_coefficients);

    return sparse_conv2d_forward_cuda(input, dictionary, lookup_indices, lookup_coefficients, stride, padding);
}

std::vector<torch::Tensor> sparse_conv2d_backward(
    torch::Tensor grad_output,
    torch::Tensor input,
    torch::Tensor dictionary,
    torch::Tensor lookup_indices,
    torch::Tensor lookup_coefficients,
    int stride, int padding) {
    
    CHECK_INPUT(grad_output);
    CHECK_INPUT(input);
    CHECK_INPUT(dictionary);
    CHECK_INPUT(lookup_indices);
    CHECK_INPUT(lookup_coefficients);

    return sparse_conv2d_backward_cuda(grad_output, input, dictionary, lookup_indices, lookup_coefficients, stride, padding);
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("forward", &sparse_conv2d_forward, "Sparse Conv2d forward (CUDA)");
    m.def("backward", &sparse_conv2d_backward, "Sparse Conv2d backward (CUDA)");
}
